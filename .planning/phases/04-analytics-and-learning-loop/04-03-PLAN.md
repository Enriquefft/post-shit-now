---
phase: 04-analytics-and-learning-loop
plan: 03
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - src/learning/preference-model.ts
  - src/learning/adjustments.ts
  - src/learning/feedback.ts
  - src/learning/locks.ts
  - content/strategy.yaml
autonomous: true
requirements: [LEARN-02, LEARN-03, LEARN-04, LEARN-05, LEARN-06, LEARN-07]

must_haves:
  truths:
    - "Preference model aggregates engagement signals and edit patterns into actionable learnings"
    - "Edit signals from edit_history are aggregated into the preference model during weekly update"
    - "System prompts explicit feedback only at key moments: 3x above average, significant underperformance, high/low edit streaks"
    - "Small adjustments auto-apply to strategy.yaml; large changes queue for user approval"
    - "A transparent changelog shows all autonomous changes made this cycle"
    - "User can lock settings permanently; locked settings are never auto-adjusted"
    - "Unlocking requires explicit action (no auto-expiry)"
  artifacts:
    - path: "src/learning/preference-model.ts"
      provides: "Preference model CRUD, weekly update logic, edit signal aggregation"
      exports: ["getPreferenceModel", "updatePreferenceModel"]
    - path: "src/learning/adjustments.ts"
      provides: "Tiered autonomous adjustment engine with auto-apply and approval queue"
      exports: ["computeAdjustments", "applyAutoAdjustments"]
    - path: "src/learning/feedback.ts"
      provides: "Explicit feedback prompt detection at key moments"
      exports: ["detectFeedbackMoments"]
    - path: "src/learning/locks.ts"
      provides: "User override lock management with permanent locks"
      exports: ["lockSetting", "unlockSetting", "isSettingLocked"]
    - path: "content/strategy.yaml"
      provides: "Git-tracked strategy config template"
      contains: "pillars"
  key_links:
    - from: "src/learning/preference-model.ts"
      to: "src/core/db/schema.ts"
      via: "preferenceModel table for read/write"
      pattern: "preferenceModel"
    - from: "src/learning/adjustments.ts"
      to: "content/strategy.yaml"
      via: "read/modify/write strategy file"
      pattern: "strategy\\.yaml"
    - from: "src/learning/adjustments.ts"
      to: "src/learning/locks.ts"
      via: "check locks before any adjustment"
      pattern: "isSettingLocked"
    - from: "src/learning/adjustments.ts"
      to: "src/core/db/schema.ts"
      via: "strategyAdjustments table for changelog"
      pattern: "strategyAdjustments"
---

<objective>
Build the learning loop: preference model that learns from engagement and edit data, autonomous adjustment engine with tiered apply model, explicit feedback detection, and user override locks.

Purpose: This is the intelligence that makes content improve over time. The preference model synthesizes signals, the adjustment engine acts on them within bounded constraints, and locks give the user ultimate control.
Output: Complete learning subsystem — preference model, adjustments, feedback, locks, strategy config.
</objective>

<execution_context>
@/home/hybridz/.claude/get-shit-done/workflows/execute-plan.md
@/home/hybridz/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-analytics-and-learning-loop/04-RESEARCH.md
@.planning/phases/04-analytics-and-learning-loop/04-01-SUMMARY.md
@src/core/db/schema.ts
@src/voice/calibration.ts
@src/content/generate.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Preference model CRUD + weekly update + edit signal aggregation</name>
  <files>
    src/learning/preference-model.ts
    src/learning/feedback.ts
    content/strategy.yaml
  </files>
  <action>
**Strategy config template (content/strategy.yaml):**

Create the initial strategy.yaml with structure:
```yaml
# Strategy Configuration — managed by learning loop
# Manual edits are fine; the system uses atomic field updates
pillars:
  # Populated from voice profile during setup
  - name: ""
    weight: 0.0  # 0.0-1.0, must sum to 1.0
posting:
  frequency:
    x: 7  # posts per week
  preferred_times:
    x: ["09:00", "12:00", "17:00"]  # UTC
  timezone: "UTC"
formats:
  preferences: []  # ranked format list, populated from learnings
locked: []  # field paths that are permanently locked by user
```

**Preference model (src/learning/preference-model.ts):**

`getPreferenceModel(db, userId)`: Query preferenceModel table by userId. Return row or null.

`createPreferenceModel(db, userId)`: Insert default row with empty arrays for all jsonb fields.

`updatePreferenceModel(db, userId, updates)`: Partial update of preferenceModel fields. Use Drizzle's `set()`.

`computeWeeklyUpdate(db, userId)`: The core weekly learning function called during /psn:review. Does:

1. **Aggregate engagement signals (LEARN-01, LEARN-04):**
   - Query postMetrics for this user's posts from last 7 days
   - Group by format: compute average engagementScore per format -> update `topFormats`
   - Group by pillar: compute average engagementScore per pillar -> update `topPillars`
   - Group by hour+dayOfWeek of publishedAt: compute average score -> update `bestPostingTimes`
   - Only update rankings if >= 3 posts exist for that dimension (avoid noise from thin data)

2. **Aggregate edit signals (LEARN-02):**
   - Query editHistory for this user from last 7 days
   - Compute average editRatio -> update `avgEditRatio`
   - Aggregate editPatterns: count occurrences of each pattern type -> update `commonEditPatterns`
   - This wires the existing edit_history table (built in Phase 3) into the preference model

3. **Return update summary** for rendering in review: `{ formatsUpdated, pillarsUpdated, timesUpdated, editPatternsUpdated }`.

**Feedback detection (src/learning/feedback.ts):**

`detectFeedbackMoments(db, userId)`: Called during /psn:review to find posts that deserve explicit feedback prompts per LEARN-03 and user decision:

1. Query postMetrics for last 7 days, compute average score
2. Find posts scoring >= 3x the average -> "What made this work so well?"
3. Find posts scoring significantly below average (< 0.3x) -> "What went wrong here?"
4. Query editHistory, check for edit streaks:
   - High edit streak: last 3+ posts all > 50% edit ratio -> "You've been heavily editing. Want to adjust voice settings?"
   - Low edit streak: last 5+ posts all < 10% edit ratio -> "Voice calibration is excellent!"
5. Return `FeedbackMoment[]` with type, postId, message, and optional action suggestion

Do NOT prompt feedback on every post — only these key moments per user decision.
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors.
    Confirm content/strategy.yaml is valid YAML: `node -e "const yaml = require('yaml'); yaml.parse(require('fs').readFileSync('content/strategy.yaml','utf8'))"`.
  </verify>
  <done>
    Preference model reads/writes to DB. Weekly update aggregates engagement and edit signals. Strategy.yaml template exists. Feedback detection identifies key moments only (3x above avg, underperformance, edit streaks).
  </done>
</task>

<task type="auto">
  <name>Task 2: Autonomous adjustments engine + user override locks</name>
  <files>
    src/learning/adjustments.ts
    src/learning/locks.ts
  </files>
  <action>
**User override locks (src/learning/locks.ts):**

`lockSetting(db, userId, field, value)`: Add entry to preferenceModel.lockedSettings jsonb array. Each entry: `{ field: string, value: unknown, lockedAt: string }`. If field already locked, update value and lockedAt.

`unlockSetting(db, userId, field)`: Remove entry from lockedSettings array by field name. Per user decision: unlocking requires explicit action, no auto-expiry.

`isSettingLocked(lockedSettings, field)`: Pure function — check if field exists in locked array. Used by adjustments engine before making changes.

`getLockedSettings(db, userId)`: Return all locked settings for display.

**Autonomous adjustments (src/learning/adjustments.ts):**

Define auto-apply rules per user decision and research:

```typescript
const AUTO_APPLY_RULES: Record<string, (delta: number) => boolean> = {
  pillar_weight: (delta) => Math.abs(delta) <= 0.05,      // +/-5% per cycle
  posting_time: (shift) => Math.abs(shift) <= 2,            // +/-2 hours
  format_preference: () => true,                             // always auto
  frequency: (delta) => Math.abs(delta) <= 1,                // +/-1/week
  new_pillar: () => false,                                   // always approval
  drop_format: () => false,                                  // always approval
};
```

Speed limits per research discretion recommendation: max one adjustment per field per week, require >= 5 posts of data before any adjustment, require >= 3 weeks of data before pillar weight changes.

`computeAdjustments(preferenceModel, currentStrategy)`: Analyze preference model learnings against current strategy.yaml values. For each dimension where data suggests a change:
- Compare top performing pillars vs current weights -> suggest weight adjustments
- Compare best posting times vs current schedule -> suggest time shifts
- Compare top formats vs current preferences -> suggest reordering
- Check frequency: if engagement per post is increasing, suggest +1/week (cap at platform-appropriate limits)
- For each suggestion, classify tier ("auto" or "approval") using AUTO_APPLY_RULES
- Check isSettingLocked for each field before including in suggestions
- Return `StrategyAdjustment[]` (matches the strategyAdjustments DB table shape)

`applyAutoAdjustments(adjustments, strategyPath)`:
- Filter adjustments to tier="auto" only
- Read strategy.yaml, apply each adjustment to the specific field (atomic per-field update, not full rewrite per research anti-pattern guidance)
- Write strategy.yaml back using `yaml` library with the atomic write (.tmp + rename) pattern from voice profile module
- Insert all adjustments (both auto and approval) into strategyAdjustments table with status "applied" for auto, "pending" for approval
- Return `{ applied: StrategyAdjustment[], queued: StrategyAdjustment[] }`

`getRecentChangelog(db, userId, since)`: Query strategyAdjustments table for changes since date. Format as structured data for rendering in review: "What the brain changed this week" per LEARN-06.

`approveAdjustment(db, adjustmentId)` and `rejectAdjustment(db, adjustmentId)`: Update status in strategyAdjustments table. If approved, also apply to strategy.yaml.
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors.
    Verify locks.ts exports: lockSetting, unlockSetting, isSettingLocked, getLockedSettings.
    Verify adjustments.ts exports: computeAdjustments, applyAutoAdjustments, getRecentChangelog.
  </verify>
  <done>
    Tiered adjustment engine: small changes auto-apply, large changes queue for approval. Locked settings are never adjusted. Changelog tracks all changes with evidence. Speed limits prevent oscillation. Strategy.yaml updated atomically.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` compiles without errors
- content/strategy.yaml is valid YAML and contains pillars, posting, formats, locked sections
- Lock/unlock cycle works: lock a field, isSettingLocked returns true, unlock, returns false
- Adjustments respect locks and speed limits
- Changelog query returns recent adjustments
</verification>

<success_criteria>
- Preference model weekly update aggregates engagement + edit signals
- Feedback prompts only at key moments (3x avg, underperformance, edit streaks)
- Auto-apply for small adjustments, approval queue for large changes
- User overrides are permanent via explicit lock
- Transparent changelog for all autonomous changes
- Strategy.yaml updated atomically with structured per-field modifications
</success_criteria>

<output>
After completion, create `.planning/phases/04-analytics-and-learning-loop/04-03-SUMMARY.md`
</output>
